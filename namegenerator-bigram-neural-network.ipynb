{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d319a6-c641-4faa-ac9f-86b85fed9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "211021e0-85bd-44f8-bbae-582294dfa336",
   "metadata": {},
   "source": [
    "# Bigram-based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8555276-cf21-43c8-8d1c-20bbdf6341cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt', 'r').read().splitlines()\n",
    "freq = {}\n",
    "\n",
    "for n in names:\n",
    "    chs = ['<S>'] + list(n) + ['<E>']\n",
    "    for ch1, ch2, in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        freq[bigram] = freq.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307335cd-48e0-488c-befe-025f5453c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((28, 28), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "for n in names:\n",
    "    chs = ['.'] + list(n) + ['.']\n",
    "    for ch1, ch2, in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e53e2-0374-4b67-b0e3-ab97eb51e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P = P / P.sum(1, keepdim=True)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(1000): \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c350ec-a10a-436e-ac30-569df282e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in [\"test\"]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b2c3d-a875-41c6-a8c2-1ff804b17025",
   "metadata": {},
   "source": [
    "# A Neural-Network Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54676c83-e992-4d84-a4f8-0bb3753ebeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for n in names:\n",
    "    chs = ['.'] + list(n) + ['.']\n",
    "    for ch1, ch2, in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "nelems = xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b44ef-e4ad-42a4-befa-7b0f4640fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): \n",
    "    # performs a forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W)\n",
    "    counts = logits.exp()\n",
    "    probs = counts /  counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(nelems), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward() # PyTorch creates compute graph to calculate derivatives\n",
    "    \n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dff96-c6aa-4d8f-873e-037ed8b13ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    \n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[ix])\n",
    "        if(ix == 0):\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
